{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the LPU Model with LDA and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries used\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Data Set Formation And Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(479, 6)\n",
      "956 500\n",
      "(479, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rushi/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "/home/rushi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reading comments and synopsis files\n",
    "dataset_file=\"dataset_shawshank.csv\"\n",
    "synopsis_file='synopsis_shawshank.txt'\n",
    "df = pd.read_csv(dataset_file)\n",
    "file = open(synopsis_file,\"r\")\n",
    "text=file.read()\n",
    "sentences = text.split('. ')\n",
    "df2 = pd.DataFrame(sentences, columns=[\"review_text\"])\n",
    "df2.to_csv('synopsis3.csv', index=False)\n",
    "\n",
    "#separating the spoiler and non-spoiler comments into two data frames\n",
    "df_pos=df.loc[df['is_spoiler'] == True]\n",
    "df_neg=df.loc[df['is_spoiler'] == False]\n",
    "pos_size=min(1000,df_pos.shape[0])\n",
    "neg_size=min(500,df_neg.shape[0])\n",
    "\n",
    "# selecting half of spoiler comments from total spoiler comments as positive dataset\n",
    "df_pos1=(df_pos.loc[:0.5*pos_size,:])\n",
    "df_pos1.drop([\"is_spoiler\"],axis=1,inplace=True)\n",
    "train_size=0.5*(pos_size)\n",
    "test_size=0.5*(pos_size+neg_size)\n",
    "df_test_pos=df_pos.loc[0.5*pos_size+1:,:]\n",
    "df_test_neg=df_neg.loc[:,:]\n",
    "\n",
    "# training data includes synopsis and half of spoiler comments\n",
    "# test data includes half of spoiler comments and all the non-spoiler comments\n",
    "frames_train=[df_pos1,df2]\n",
    "frames_test=[df_test_pos,df_test_neg]\n",
    "df_train=pd.concat(frames_train)\n",
    "df_test=pd.concat(frames_test)\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving index to the comments.\n",
    "\n",
    "index=[]\n",
    "for i in range(len(df_train.index)):\n",
    "    index.append(\"Positive \"+str(i))\n",
    "df_train['index']=index\n",
    "df_train.set_index('index', inplace=True)\n",
    "\n",
    "index2=[]\n",
    "for i in range(len(df_test.index)):\n",
    "    index2.append(\"Comment \"+str(i))\n",
    "df_test['index']=index2\n",
    "df_test.set_index('index', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the input comments i.e. removing punctuation marks and digits\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned output data\n",
    "\n",
    "data_clean = pd.DataFrame(df_train.review_text.apply(round1))\n",
    "data_clean2 = pd.DataFrame(df_test.review_text.apply(round1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used CountVectorizer to get count of all the words present in the comment \n",
    "# It is used for preparing feature vector and training and test set\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.review_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "\n",
    "cv2 = CountVectorizer(stop_words='english')\n",
    "data_cv2 = cv2.fit_transform(data_clean2.review_text)\n",
    "data_dtm2 = pd.DataFrame(data_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "data_dtm2.index = data_clean2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing libraries\n",
    "\n",
    "import numpy as np\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus formation for lda input.\n",
    "sparse_counts = scipy.sparse.csr_matrix(data_dtm.transpose())\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "# word to identifier(a number) dictionary\n",
    "id_word_dict={}\n",
    "for i in range(len(data_dtm.columns)):\n",
    "    id_word_dict[i]=data_dtm.columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  applied LDA on corpus. \n",
    "total_topics=50\n",
    "lda = models.LdaMulticore(corpus=corpus, id2word=id_word_dict, num_topics=total_topics, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word to index dictionary\n",
    "# Indexed all words in data_dtm and data_dtm2 matrix\n",
    "word_dict={}\n",
    "word_dict2={}\n",
    "j=0\n",
    "for i in data_dtm.columns:\n",
    "    word_dict[i]=j\n",
    "    j+=1\n",
    "j=0\n",
    "for i in data_dtm2.columns:\n",
    "    word_dict2[i]=j\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped words from lda topic distribution output\n",
    "# Stored in dictionary of form word-index\n",
    "word_weight_positive={}\n",
    "cnt=0\n",
    "for i in range(total_topics):\n",
    "    word_topic=lda.print_topic(i).split('+')\n",
    "    j=0\n",
    "    for word_probab in word_topic:\n",
    "        val,wor=word_probab.split('*')        \n",
    "        wor=str(wor)\n",
    "        #print(wor)\n",
    "        y=len(wor)\n",
    "        if j==len(word_topic)-1:\n",
    "            wor = wor[1:y-1]\n",
    "        else:\n",
    "            wor = wor[1:y-2]\n",
    "        #print(wor)\n",
    "        if wor not in word_weight_positive:\n",
    "            word_weight_positive[wor]=cnt\n",
    "            cnt+=1\n",
    "            \n",
    "        \n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rushi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imported some libraries for similarity checking.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Preparation of Training and Test  Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "# Initializing training and test set as all zeros \n",
    "# Here number of features for each training sample is\n",
    "# number of words LDA gave\n",
    "\n",
    "train_size = len(df_train.index)\n",
    "test_size = len(df_test.index)\n",
    "no_of_words = len(word_weight_positive)\n",
    "X_train = np.zeros((train_size,no_of_words))\n",
    "X_test = np.zeros((test_size,no_of_words))\n",
    "Y_test = np.zeros(test_size)\n",
    "print(len(word_weight_positive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each comment (in training set) we were iterating over words of each comment.\n",
    "# If word was not found in lda-output words , we were searching weather  similar\n",
    "# word  from set given by 'Wordnet' is present in lda-output or not.\n",
    "# If we found similar word is present then we are updating weight of that feature\n",
    "# with weight of current word.\n",
    "\n",
    "\n",
    "#Finaly we have created matrix input for training one class svm.\n",
    "for i in range(train_size):\n",
    "    s = set(data_clean.iloc[i][0].split(' '))\n",
    "    for wor in s:\n",
    "        if wor not in word_weight_positive:\n",
    "            flag=0\n",
    "            for syn in wordnet.synsets(wor): \n",
    "                for l in syn.lemmas(): \n",
    "                    temp_word = l.name()\n",
    "                    if temp_word in word_weight_positive:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==1:\n",
    "                    break\n",
    "            if flag==1:\n",
    "                index = word_weight_positive[temp_word]\n",
    "                ind2 = word_dict[temp_word]\n",
    "                X_train[i][index]+=data_dtm.iloc[i][ind2]\n",
    "        else :\n",
    "            index = word_weight_positive[wor]\n",
    "            ind2 = word_dict[wor]\n",
    "            X_train[i][index]+=data_dtm.iloc[i][ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN the same way as we have created testing matrix\n",
    "for i in range(test_size):\n",
    "    s = set(data_clean2.iloc[i][0].split(' '))\n",
    "    for wor in s:\n",
    "        if wor in word_dict2:\n",
    "            if wor not in word_weight_positive:\n",
    "                flag=0\n",
    "                for syn in wordnet.synsets(wor): \n",
    "                    for l in syn.lemmas(): \n",
    "                        temp_word = l.name()\n",
    "                        if temp_word in word_weight_positive:\n",
    "                            flag=1\n",
    "                            break\n",
    "                    if flag==1:\n",
    "                        break\n",
    "                if flag==1:\n",
    "                    index = word_weight_positive[temp_word]\n",
    "                    ind2 = word_dict2[wor]\n",
    "                    X_test[i][index]+=data_dtm2.iloc[i][ind2]\n",
    "            else :\n",
    "                index = word_weight_positive[wor]\n",
    "                ind2 = word_dict2[wor]\n",
    "                X_test[i][index]+=data_dtm2.iloc[i][ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the labels of test data\n",
    "\n",
    "for i in range(test_size):\n",
    "    Y_test[i]=df_test.iloc[i][\"is_spoiler\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. One Class SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Applied One class SVM model.\n",
    "# you can change gamma attribute with polynomial and give degree as input.\n",
    "from sklearn.svm import OneClassSVM\n",
    "lpu_model = OneClassSVM(gamma='auto').fit(X_train)\n",
    "Y_out = lpu_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Reslult Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result calculations.\n",
    "\n",
    "total=len(Y_out)\n",
    "false_positive=0\n",
    "false_negative=0\n",
    "true_positive=0\n",
    "true_negative=0\n",
    "\n",
    "for i in range(len(Y_out)):\n",
    "    if Y_out[i]==-1 and Y_test[i]==1:\n",
    "        false_negative+=1\n",
    "    elif Y_out[i]==1 and Y_test[i]==0:\n",
    "        false_positive+=1\n",
    "    elif Y_out[i]==1 and Y_test[i]==1:\n",
    "        true_positive+=1\n",
    "    else:\n",
    "        true_negative+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result declaration\n",
    "\n",
    "print(true_positive,true_negative)\n",
    "print(false_negative,false_positive)\n",
    "print((true_negative+true_positive)/total)\n",
    "\n",
    "precision=true_positive/(true_positive+false_positive)\n",
    "recall=true_positive/(true_positive+false_negative)\n",
    "f1score=(2*precision*recall)/(precision+recall)\n",
    "print(f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
