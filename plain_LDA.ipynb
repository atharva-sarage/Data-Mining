{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal LDA Method \n",
    "Classifying comments by comparing cosine similarities of probabilities obtained by LDA on synopsis and comments \n",
    "with particular threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import string \n",
    "import sklearn \n",
    "import csv\n",
    "import numpy as np\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments dataset path (just change file name for different movies testing)\n",
    "dataset_file=\"dataset_shawshank.csv\"\n",
    "#synopsis text file path (just change file name for different movies testing)\n",
    "synopsis_file=\"synopsis_shawshank.txt\"\n",
    "df = pd.read_csv(dataset_file)\n",
    "file = open(synopsis_file,\"r\")\n",
    "text=file.read()\n",
    "sentences = text.split('. ')\n",
    "   \n",
    "df2 = pd.DataFrame(sentences, columns=[\"synopsis\"])\n",
    "df2.to_csv('synopsis3.csv', index=False)\n",
    "# print(df2.head())\n",
    "\n",
    "#Splitting dataset into train and test in 8:2 ratio\n",
    "df_pos=df.loc[df['is_spoiler'] == True]\n",
    "df_neg=df.loc[df['is_spoiler'] == False]\n",
    "pos_size=min(1000,df_pos.shape[0])\n",
    "neg_size=min(1500,df_neg.shape[0])\n",
    "# print(pos_size,neg_size)\n",
    "train_size=0.8*(pos_size+neg_size)\n",
    "test_size=0.2*(pos_size+neg_size)\n",
    "df_train_pos=df_pos.loc[:0.8*pos_size,:]\n",
    "df_test_pos=df_pos.loc[0.8*pos_size+1:,:]\n",
    "df_train_neg=df_neg.loc[:0.8*neg_size,:]\n",
    "df_test_neg=df_neg.loc[0.8*neg_size+1:,:]\n",
    "# df_train_neg.to_csv('train.csv', index=False)\n",
    "frames_train=[df_train_pos,df_train_neg]\n",
    "frames_test=[df_test_pos,df_test_neg]\n",
    "df_train=pd.concat(frames_train)\n",
    "df_test=pd.concat(frames_test)\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size=len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending Comment before index number in index column\n",
    "index=[]\n",
    "for i in range(len(df_train.index)):\n",
    "    index.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df_train['index']=index\n",
    "df_train.set_index('index', inplace=True)\n",
    "\n",
    "index2=[]\n",
    "for i in range(len(df2.index)):\n",
    "    index2.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df2['index']=index2\n",
    "df2.set_index('index', inplace=True)\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing \n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning both comments and synopsis data\n",
    "data_clean = pd.DataFrame(df_train.review_text.apply(round1))\n",
    "data_clean2 = pd.DataFrame(df2.synopsis.apply(round1))\n",
    "# data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Creating document term matrix for comments data with each entry denoting the count of the term in the comment\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.review_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "#Creating document term matrix for synopsis data with each entry denoting the count of the term in the comment\n",
    "cv2 = CountVectorizer(stop_words='english')\n",
    "data_cv2 = cv2.fit_transform(data_clean2.synopsis)\n",
    "data_dtm2 = pd.DataFrame(data_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "data_dtm2.index = data_clean2.index\n",
    "# data_dtm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating id to word dictionary for comments data\n",
    "id_word_dict={}\n",
    "for i in range(len(data_dtm.columns)):\n",
    "    id_word_dict[i]=data_dtm.columns[i]\n",
    "# print(id_word_dict)\n",
    "tdm = data_dtm.transpose()\n",
    "\n",
    "#Creating id to word dictionary for synopsis data\n",
    "id_word_dict2={}\n",
    "for i in range(len(data_dtm2.columns)):\n",
    "    id_word_dict2[i]=data_dtm2.columns[i]\n",
    "# print(id_word_dict2)\n",
    "tdm2 = data_dtm2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the term document matrix in sparse matrix for preparing corpus of comments data\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "#Converting the term document matrix in sparse matrix for preparing corpus of synopsis data\n",
    "sparse_counts2 = scipy.sparse.csr_matrix(tdm2)\n",
    "corpus2 = matutils.Sparse2Corpus(sparse_counts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA on both comments and synopsis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics=20\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id_word_dict, num_topics=total_topics, passes=50)\n",
    "#lda.print_topics()\n",
    "lda2 = models.LdaModel(corpus=corpus2, id2word=id_word_dict2, num_topics=total_topics, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda.print_topics()\n",
    "#lda2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing dictionary of important words obtained by LDA on comments data and storing the maximum probability\n",
    "#of that word occuring in any topic\n",
    "word_weight={}\n",
    "for i in range(total_topics):\n",
    "    word_topic=lda.print_topic(i).split('+')\n",
    "    j=0\n",
    "    for word_probab in word_topic:\n",
    "        val,wor=word_probab.split('*')\n",
    "        wor=str(wor)\n",
    "#         print(wor)\n",
    "        y=len(wor)\n",
    "        if j==len(word_topic)-1:\n",
    "            wor = wor[1:y-1]\n",
    "        else:\n",
    "            wor = wor[1:y-2]\n",
    "#         print(wor)\n",
    "        if wor in word_weight:\n",
    "            if word_weight[wor]<val:\n",
    "                word_weight[wor]=val\n",
    "        else :\n",
    "             word_weight[wor]=val\n",
    "        \n",
    "        j=j+1\n",
    "\n",
    "# print(word_weight)\n",
    "# print(word_weight['start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing dictionary of important words obtained by LDA on synopsis data and storing the maximum probability\n",
    "#of that word occuring in any topic\n",
    "word_weight_synopsis={}\n",
    "for i in range(total_topics):\n",
    "    word_topic2=lda2.print_topic(i).split('+')\n",
    "    j=0\n",
    "    for word_probab in word_topic2:\n",
    "        val,wor=word_probab.split('*')        \n",
    "        wor=str(wor)\n",
    "        #print(wor)\n",
    "        y=len(wor)\n",
    "        if j==len(word_topic2)-1:\n",
    "            wor = wor[1:y-1]\n",
    "        else:\n",
    "            wor = wor[1:y-2]\n",
    "        #print(wor)\n",
    "        if wor in word_weight_synopsis:\n",
    "            if word_weight_synopsis[wor]<val:\n",
    "                word_weight_synopsis[wor]=val\n",
    "        else :\n",
    "             word_weight_synopsis[wor]=val\n",
    "        \n",
    "        j=j+1\n",
    "        \n",
    "#print(word_weight_synopsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating max probability of words of comments as well as synopsis and storing in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading training dataset\n",
    "df3=pd.read_csv('test.csv')\n",
    "total_size=df3.shape[0]\n",
    "#total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_list is the list of cosine similarity score of comments of test data\n",
    "output_list=[]\n",
    "#threshold2 is the probability with which a word of comment should occur in synopsis' LDA\n",
    "#so that the comment matches more with synopsis\n",
    "threshold2=0.01\n",
    "counter=0\n",
    "for j in range(total_size):\n",
    "    comment_t=df3['review_text'][j]\n",
    "    if df3['is_spoiler'][j] == 0:\n",
    "        counter+=1\n",
    "    #cleaning the new comment of test data\n",
    "    comment_t=clean_text_round1(comment_t)\n",
    "    s1=comment_t.split(\" \")\n",
    "    comment_ts=set(s1)\n",
    "    comment_ts\n",
    "    sum=0\n",
    "    sum2=0\n",
    "    sum3=0\n",
    "    #normalizing quantity\n",
    "    for i in word_weight_synopsis.values():\n",
    "        sum3+=float(i)*float(i)  \n",
    "    \n",
    "    #calculating dot product of probabilities of word of new comment if it\n",
    "    #occurs in both comments' LDA as well as synopsis' LDA \n",
    "    for i in comment_ts:\n",
    "        #normalizing quantity\n",
    "        if i in word_weight :\n",
    "            sum2+=float(word_weight[i])*float(word_weight[i])\n",
    "        #dot product\n",
    "        if i in word_weight_synopsis and i in word_weight and float(word_weight_synopsis[i])>threshold2:\n",
    "            sum+=float(word_weight_synopsis[i])*float(word_weight[i])                     \n",
    "    if sum2!=0 :\n",
    "        output_list.append(sum/(np.sqrt(sum2)*np.sqrt(sum3)))\n",
    "    else:\n",
    "        output_list.append(0)\n",
    "#print(counter)  \n",
    "#output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['output']=output_list\n",
    "df3.to_csv('final.csv')\n",
    "df4=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to caculate evaluation metrics such as TP,TN,FP,FN,Recall,Precision,F1-score\n",
    "def solve(total_size,df3,threshold):\n",
    "#   total=0\n",
    "    false_positive=0\n",
    "    false_negative=0\n",
    "    true_positive=0\n",
    "    true_negative=0\n",
    "#   threshold=0.11\n",
    "\n",
    "    for i in range(total_size):\n",
    "        #print(df3['output'][i],threshold,df3['Y'][i])\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==True  :\n",
    "            true_positive+=1\n",
    "        if  df3['output'][i]<threshold and df3['is_spoiler'][i]==False :\n",
    "            true_negative+=1\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==False:\n",
    "            false_positive+=1\n",
    "        if df3['output'][i]<threshold and df3['is_spoiler'][i]==True:\n",
    "            false_negative+=1\n",
    "\n",
    "#   print(true_positive,true_negative)\n",
    "#   print(false_negative,false_positive)\n",
    "#   print((true_negative+true_positive)/total_size)\n",
    "    accuracy = (true_positive+true_negative)/(total_size)\n",
    "    precision=true_positive/(true_positive+false_positive)\n",
    "    recall=true_positive/(true_positive+false_negative)\n",
    "    f1score=(2*precision*recall)/(precision+recall)\n",
    "    return (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)\n",
    "#   print(f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of evaluation metrics for different values of threshold hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013134328358208954 1.0 0.44 0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "# evaluation metrices for top K comments.(The comments are ranked based on cosine similarity values)\n",
    "df4.sort_values(by='output', ascending=False,inplace=True)\n",
    "top_k=100\n",
    "false_positive=0\n",
    "false_negative=0\n",
    "true_positive=0\n",
    "true_negative=0\n",
    "best_threshold=0.11  # add best threshold obtained\n",
    "for i in range(top_k):\n",
    "    if  df4['output'][i]>best_threshold and df4['is_spoiler'][i]==True  :\n",
    "        true_positive+=1\n",
    "    if  df4['output'][i]<best_threshold and df4['is_spoiler'][i]==False :\n",
    "        true_negative+=1\n",
    "    if  df4['output'][i]>threshold and df4['is_spoiler'][i]==False:\n",
    "        false_positive+=1\n",
    "    if  df4['output'][i]<threshold and df4['is_spoiler'][i]==True:\n",
    "        false_negative+=1\n",
    "\n",
    "accuracy = (true_positive+true_negative)/(total_size)\n",
    "precision=true_positive/(true_positive+false_positive)\n",
    "recall=true_positive/(true_positive+false_negative)\n",
    "f1score=(2*precision*recall)/(precision+recall)\n",
    "print(accuracy,precision,recall,f1score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6143283582089553, 0.6411940298507462, 0.6704477611940298, 0.6940298507462687, 0.7092537313432836, 0.7185074626865672, 0.7244776119402985, 0.7313432835820896, 0.7349253731343284, 0.7367164179104477]\n",
      "[0.1599479843953186, 0.1629526462395543, 0.1649016641452345, 0.1700404858299595, 0.17176870748299317, 0.17353198948290974, 0.17515638963360142, 0.1773308957952468, 0.17929759704251386, 0.18029739776951673]\n"
     ]
    }
   ],
   "source": [
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "f1 = []\n",
    "acc =[]\n",
    "threshold=0.08\n",
    "#threshold2=0.02\n",
    "x=0\n",
    "for x in range(10):\n",
    "    #print(word_weight_synopsis)\n",
    "    (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)= solve(total_size,df3,threshold)\n",
    "#   print(total_size)\n",
    "    tp.append(true_positive)\n",
    "    tn.append(false_positive)\n",
    "    fp.append(true_negative)\n",
    "    fn.append(false_negative)\n",
    "    f1.append(f1score)\n",
    "    acc.append(accuracy)\n",
    "    threshold+=0.01\n",
    "\n",
    "print(acc)\n",
    "print(f1)\n",
    "sample = open('plain-lda-darkknight_result.txt',\"a\")\n",
    "print(\"iteration:\\n\",x,file = sample)\n",
    "print(\"\\ntopic:\",total_topics,file=sample)\n",
    "print( \"\\naccuracy:\\n\",acc , file = sample)\n",
    "print( \"\\ntrue_positive:\\n\",tp , file = sample)\n",
    "print( \"\\nfalse_positive:\\n\",fp , file = sample)\n",
    "print( \"\\ntrue_negative:\\n\",tn , file = sample)\n",
    "print( \"\\nfalse_negative:\\n\",fn , file = sample)\n",
    "print( \"\\nf1-score:\\n\",f1 ,file = sample)\n",
    "sample.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
