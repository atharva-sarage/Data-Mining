{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1500\n"
     ]
    }
   ],
   "source": [
    "dataset_file=\"dataset7(dark knight).csv\"\n",
    "synopsis_file='synopsis_darkknight.txt'\n",
    "df = pd.read_csv(dataset_file)\n",
    "file = open(synopsis_file,\"r\")\n",
    "text=file.read()\n",
    "sentences = text.split('. ')\n",
    "   \n",
    "df2 = pd.DataFrame(sentences, columns=[\"synopsis\"])\n",
    "df2.to_csv('synopsis3.csv', index=False)\n",
    "# print(df2.head())\n",
    "df_pos=df.loc[df['is_spoiler'] == True]\n",
    "df_neg=df.loc[df['is_spoiler'] == False]\n",
    "pos_size=min(1000,df_pos.shape[0])\n",
    "neg_size=min(1500,df_neg.shape[0])\n",
    "print(pos_size,neg_size)\n",
    "train_size=0.8*(pos_size+neg_size)\n",
    "test_size=0.2*(pos_size+neg_size)\n",
    "df_train_pos=df_pos.loc[:0.8*pos_size,:]\n",
    "df_test_pos=df_pos.loc[0.8*pos_size+1:,:]\n",
    "df_train_neg=df_neg.loc[:0.8*neg_size,:]\n",
    "df_test_neg=df_neg.loc[0.8*neg_size+1:,:]\n",
    "# df_train_neg.to_csv('train.csv', index=False)\n",
    "frames_train=[df_train_pos,df_train_neg]\n",
    "frames_test=[df_test_pos,df_test_neg]\n",
    "df_train=pd.concat(frames_train)\n",
    "df_test=pd.concat(frames_test)\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size=len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in range(len(df_train.index)):\n",
    "    index.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df_train['index']=index\n",
    "df_train.set_index('index', inplace=True)\n",
    "\n",
    "index2=[]\n",
    "for i in range(len(df2.index)):\n",
    "    index2.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df2['index']=index2\n",
    "df2.set_index('index', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(df_train.review_text.apply(round1))\n",
    "data_clean2 = pd.DataFrame(df2.synopsis.apply(round1))\n",
    "# data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 1357)\n",
      "(801, 16868)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.review_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "\n",
    "cv2 = CountVectorizer(stop_words='english')\n",
    "data_cv2 = cv2.fit_transform(data_clean2.synopsis)\n",
    "data_dtm2 = pd.DataFrame(data_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "data_dtm2.index = data_clean2.index\n",
    "print(data_dtm2.shape)\n",
    "print(data_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_word_dict={}\n",
    "for i in range(len(data_dtm.columns)):\n",
    "    id_word_dict[i]=data_dtm.columns[i]\n",
    "#print(id_word_dict)\n",
    "tdm = data_dtm.transpose()\n",
    "\n",
    "id_word_dict2={}\n",
    "for i in range(len(data_dtm2.columns)):\n",
    "    id_word_dict2[i]=data_dtm2.columns[i]\n",
    "# print(id_word_dict2)\n",
    "tdm2 = data_dtm2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "sparse_counts2 = scipy.sparse.csr_matrix(tdm2)\n",
    "corpus2 = matutils.Sparse2Corpus(sparse_counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(word_weight,word_weight_synopsis,threshold,threshold2):\n",
    "    df3=pd.read_csv(\"test.csv\")\n",
    "    total_size=df3.shape[0]\n",
    "    print(total_size)\n",
    "    \n",
    "    output_list=[]\n",
    "    counter=0\n",
    "    for j in range(total_size):\n",
    "       # print(total_size-j)\n",
    "        comment_t=df3['review_text'][j]\n",
    "        if df3['is_spoiler'][j] == 0:\n",
    "            counter+=1\n",
    "\n",
    "        comment_t=clean_text_round1(comment_t)\n",
    "        s1=comment_t.split(\" \")\n",
    "        comment_ts=set(s1)\n",
    "        comment_ts\n",
    "        sum=0\n",
    "        sum2=0\n",
    "        sum3=0\n",
    "        for i in word_weight_synopsis.values():\n",
    "            sum3+=float(i)*float(i)  \n",
    "\n",
    "        for wor in comment_ts:\n",
    "            temp_max=0\n",
    "            synonyms = []\n",
    "            synonyms.append(wor);\n",
    "            for syn in wordnet.synsets(wor): \n",
    "                for l in syn.lemmas(): \n",
    "                    synonyms.append(l.name()) \n",
    "            flag=False\n",
    "            found_word=\"\";\n",
    "            for i in synonyms: \n",
    "                if i in word_weight :\n",
    "                    found_word=i\n",
    "                    flag=True                \n",
    "                if i in word_weight_synopsis and i in word_weight and float(word_weight_synopsis[i])>threshold2:\n",
    "                    temp_max=max(temp_max,float(word_weight_synopsis[i])*float(word_weight[i]))\n",
    "                    found_word=i\n",
    "                    break\n",
    "\n",
    "            if flag==True:\n",
    "                sum2+=float(word_weight[found_word])*float(word_weight[found_word])\n",
    "                sum+=temp_max\n",
    "\n",
    "        if sum2!=0 :\n",
    "            #print(sum,sum2,sum3)\n",
    "            output_list.append(sum/(np.sqrt(sum2)*np.sqrt(sum3)))\n",
    "        else:\n",
    "            output_list.append(0)\n",
    "            \n",
    "            \n",
    "    df3['output']=output_list\n",
    "    cols = list(df3.columns.values)\n",
    "    df3 = df3[['Unnamed: 0', 'is_spoiler',  'output' ,'review_text']]\n",
    "    df3.to_csv('final.csv')\n",
    "    total=0\n",
    "    false_positive=0\n",
    "    false_negative=0\n",
    "    true_positive=0\n",
    "    true_negative=0\n",
    "\n",
    "    for i in range(total_size):\n",
    "        #print(df3['output'][i],threshold,df3['Y'][i])\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==True  :\n",
    "            true_positive+=1\n",
    "        if  df3['output'][i]<threshold and df3['is_spoiler'][i]==False :\n",
    "            true_negative+=1\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==False:\n",
    "            false_positive+=1\n",
    "        if df3['output'][i]<threshold and df3['is_spoiler'][i]==True:\n",
    "            false_negative+=1\n",
    "    \n",
    "    accuracy=(true_negative+true_positive)/total_size\n",
    "    precision=true_positive/(true_positive+false_positive)\n",
    "    recall=true_positive/(true_positive+false_negative)\n",
    "    f1score=(2*precision*recall)/(precision+recall)\n",
    "    return (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(total_topics,corpus,id_word_dict,corpus2,id_word_dict2):\n",
    "    lda = models.LdaMulticore(corpus=corpus, id2word=id_word_dict, num_topics=total_topics, passes=1)\n",
    "    lda2 = models.LdaMulticore(corpus=corpus2, id2word=id_word_dict2, num_topics=total_topics, passes=1)\n",
    "    \n",
    "    #print(lda.print_topics())\n",
    "# word weight dictionary comments\n",
    "    word_weight={}    \n",
    "    for i in range(total_topics):\n",
    "        word_topic=lda.print_topic(i).split('+')\n",
    "        j=0\n",
    "        for word_probab in word_topic:\n",
    "            val,wor=word_probab.split('*')\n",
    "            wor=str(wor)\n",
    "            y=len(wor)\n",
    "            if j==len(word_topic)-1:\n",
    "                wor = wor[1:y-1]\n",
    "            else:\n",
    "                wor = wor[1:y-2]\n",
    "            if wor in word_weight:\n",
    "                if word_weight[wor]<val:\n",
    "                    word_weight[wor]=val\n",
    "            else :\n",
    "                 word_weight[wor]=val\n",
    "            j=j+1\n",
    "            \n",
    "# word weight dictionary synopsis\n",
    "    word_weight_synopsis={}\n",
    "    for i in range(total_topics):\n",
    "        word_topic2=lda2.print_topic(i).split('+')\n",
    "        j=0\n",
    "        for word_probab in word_topic2:\n",
    "            val,wor=word_probab.split('*')        \n",
    "            wor=str(wor)\n",
    "            #print(wor)\n",
    "            y=len(wor)\n",
    "            if j==len(word_topic2)-1:\n",
    "                wor = wor[1:y-1]\n",
    "            else:\n",
    "                wor = wor[1:y-2]\n",
    "            #print(wor)\n",
    "            if wor in word_weight_synopsis:\n",
    "                if word_weight_synopsis[wor]<val:\n",
    "                    word_weight_synopsis[wor]=val\n",
    "            else :\n",
    "                 word_weight_synopsis[wor]=val\n",
    "\n",
    "            j=j+1\n",
    "            \n",
    "    # evaluation\n",
    "    return (word_weight,word_weight_synopsis)\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics=100\n",
    "passes=50\n",
    "(word_weight,word_weight_synopsis)=solve(total_topics,corpus,id_word_dict,corpus2,id_word_dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044\n"
     ]
    }
   ],
   "source": [
    "threshold=0.11\n",
    "threshold2=0.02\n",
    "#print(word_weight_synopsis)\n",
    "(true_positive,true_negative,false_positive,false_negative,f1score,accuracy)=test_model(word_weight,word_weight_synopsis,threshold,threshold2)\n",
    "print(true_positive,true_negative,false_positive,false_negative,f1score,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_positive,true_negative,false_positive,false_negative,f1score,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
