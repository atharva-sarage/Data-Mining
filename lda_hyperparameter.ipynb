{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 1500\n"
     ]
    }
   ],
   "source": [
    "dataset_file=\"dataset_godfather.csv\"\n",
    "synopsis_file='synopsis_godfather.txt'\n",
    "df = pd.read_csv(dataset_file)\n",
    "file = open(synopsis_file,\"r\")\n",
    "text=file.read()\n",
    "sentences = text.split('. ')\n",
    "   \n",
    "df2 = pd.DataFrame(sentences, columns=[\"synopsis\"])\n",
    "df2.to_csv('synopsis3.csv', index=False)\n",
    "# print(df2.head())\n",
    "df_pos=df.loc[df['is_spoiler'] == True]\n",
    "df_neg=df.loc[df['is_spoiler'] == False]\n",
    "pos_size=min(1000,df_pos.shape[0])\n",
    "neg_size=min(1500,df_neg.shape[0])\n",
    "print(pos_size,neg_size)\n",
    "train_size=0.8*(pos_size+neg_size)\n",
    "test_size=0.2*(pos_size+neg_size)\n",
    "df_train_pos=df_pos.loc[:0.8*pos_size,:]\n",
    "df_test_pos=df_pos.loc[0.8*pos_size+1:,:]\n",
    "df_train_neg=df_neg.loc[:0.8*neg_size,:]\n",
    "df_test_neg=df_neg.loc[0.8*neg_size+1:,:]\n",
    "# df_train_neg.to_csv('train.csv', index=False)\n",
    "frames_train=[df_train_pos,df_train_neg]\n",
    "frames_test=[df_test_pos,df_test_neg]\n",
    "df_train=pd.concat(frames_train)\n",
    "df_test=pd.concat(frames_test)\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size=len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in range(len(df_train.index)):\n",
    "    index.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df_train['index']=index\n",
    "df_train.set_index('index', inplace=True)\n",
    "\n",
    "index2=[]\n",
    "for i in range(len(df2.index)):\n",
    "    index2.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df2['index']=index2\n",
    "df2.set_index('index', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(df_train.review_text.apply(round1))\n",
    "data_clean2 = pd.DataFrame(df2.synopsis.apply(round1))\n",
    "# data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 948)\n",
      "(1118, 14087)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.review_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "\n",
    "cv2 = CountVectorizer(stop_words='english')\n",
    "data_cv2 = cv2.fit_transform(data_clean2.synopsis)\n",
    "data_dtm2 = pd.DataFrame(data_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "data_dtm2.index = data_clean2.index\n",
    "print(data_dtm2.shape)\n",
    "print(data_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_word_dict={}\n",
    "for i in range(len(data_dtm.columns)):\n",
    "    id_word_dict[i]=data_dtm.columns[i]\n",
    "#print(id_word_dict)\n",
    "tdm = data_dtm.transpose()\n",
    "\n",
    "id_word_dict2={}\n",
    "for i in range(len(data_dtm2.columns)):\n",
    "    id_word_dict2[i]=data_dtm2.columns[i]\n",
    "# print(id_word_dict2)\n",
    "tdm2 = data_dtm2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "sparse_counts2 = scipy.sparse.csr_matrix(tdm2)\n",
    "corpus2 = matutils.Sparse2Corpus(sparse_counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(word_weight,word_weight_synopsis,threshold,threshold2):\n",
    "    df3=pd.read_csv(\"test.csv\")\n",
    "    total_size=df3.shape[0]\n",
    "#     print(total_size)\n",
    "    \n",
    "    output_list=[]\n",
    "    counter=0\n",
    "    for j in range(total_size):\n",
    "       # print(total_size-j)\n",
    "        comment_t=df3['review_text'][j]\n",
    "        if df3['is_spoiler'][j] == 0:\n",
    "            counter+=1\n",
    "\n",
    "        comment_t=clean_text_round1(comment_t)\n",
    "        s1=comment_t.split(\" \")\n",
    "        comment_ts=set(s1)\n",
    "        comment_ts\n",
    "        sum=0\n",
    "        sum2=0\n",
    "        sum3=0\n",
    "        for i in word_weight_synopsis.values():\n",
    "            sum3+=float(i)*float(i)  \n",
    "\n",
    "        for wor in comment_ts:\n",
    "            temp_max=0\n",
    "            synonyms = []\n",
    "            synonyms.append(wor);\n",
    "            for syn in wordnet.synsets(wor): \n",
    "                for l in syn.lemmas(): \n",
    "                    synonyms.append(l.name()) \n",
    "            flag=False\n",
    "            found_word=\"\";\n",
    "            for i in synonyms: \n",
    "                if i in word_weight :\n",
    "                    found_word=i\n",
    "                    flag=True                \n",
    "                if i in word_weight_synopsis and i in word_weight and float(word_weight_synopsis[i])>threshold2:\n",
    "                    temp_max=max(temp_max,float(word_weight_synopsis[i])*float(word_weight[i]))\n",
    "                    found_word=i\n",
    "                    break\n",
    "\n",
    "            if flag==True:\n",
    "                sum2+=float(word_weight[found_word])*float(word_weight[found_word])\n",
    "                sum+=temp_max\n",
    "\n",
    "        if sum2!=0 :\n",
    "            #print(sum,sum2,sum3)\n",
    "            output_list.append(sum/(np.sqrt(sum2)*np.sqrt(sum3)))\n",
    "        else:\n",
    "            output_list.append(0)\n",
    "            \n",
    "            \n",
    "    df3['output']=output_list\n",
    "    cols = list(df3.columns.values)\n",
    "    df3 = df3[['Unnamed: 0', 'is_spoiler',  'output' ,'review_text']]\n",
    "    df3.to_csv('final.csv')\n",
    "    total=0\n",
    "    false_positive=0\n",
    "    false_negative=0\n",
    "    true_positive=0\n",
    "    true_negative=0\n",
    "\n",
    "    for i in range(total_size):\n",
    "        #print(df3['output'][i],threshold,df3['Y'][i])\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==True  :\n",
    "            true_positive+=1\n",
    "        if  df3['output'][i]<threshold and df3['is_spoiler'][i]==False :\n",
    "            true_negative+=1\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==False:\n",
    "            false_positive+=1\n",
    "        if df3['output'][i]<threshold and df3['is_spoiler'][i]==True:\n",
    "            false_negative+=1\n",
    "    \n",
    "    accuracy=(true_negative+true_positive)/total_size\n",
    "    precision=true_positive/(true_positive+false_positive)\n",
    "    recall=true_positive/(true_positive+false_negative)\n",
    "    f1score=(2*precision*recall)/(precision+recall)\n",
    "    return (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(total_topics,corpus,id_word_dict,corpus2,id_word_dict2):\n",
    "    lda = models.LdaMulticore(corpus=corpus, id2word=id_word_dict, num_topics=total_topics, passes=1)\n",
    "    lda2 = models.LdaMulticore(corpus=corpus2, id2word=id_word_dict2, num_topics=total_topics, passes=1)\n",
    "    \n",
    "    #print(lda.print_topics())\n",
    "# word weight dictionary comments\n",
    "    word_weight={}    \n",
    "    for i in range(total_topics):\n",
    "        word_topic=lda.print_topic(i).split('+')\n",
    "        j=0\n",
    "        for word_probab in word_topic:\n",
    "            val,wor=word_probab.split('*')\n",
    "            wor=str(wor)\n",
    "            y=len(wor)\n",
    "            if j==len(word_topic)-1:\n",
    "                wor = wor[1:y-1]\n",
    "            else:\n",
    "                wor = wor[1:y-2]\n",
    "            if wor in word_weight:\n",
    "                if word_weight[wor]<val:\n",
    "                    word_weight[wor]=val\n",
    "            else :\n",
    "                 word_weight[wor]=val\n",
    "            j=j+1\n",
    "            \n",
    "# word weight dictionary synopsis\n",
    "    word_weight_synopsis={}\n",
    "    for i in range(total_topics):\n",
    "        word_topic2=lda2.print_topic(i).split('+')\n",
    "        j=0\n",
    "        for word_probab in word_topic2:\n",
    "            val,wor=word_probab.split('*')        \n",
    "            wor=str(wor)\n",
    "            #print(wor)\n",
    "            y=len(wor)\n",
    "            if j==len(word_topic2)-1:\n",
    "                wor = wor[1:y-1]\n",
    "            else:\n",
    "                wor = wor[1:y-2]\n",
    "            #print(wor)\n",
    "            if wor in word_weight_synopsis:\n",
    "                if word_weight_synopsis[wor]<val:\n",
    "                    word_weight_synopsis[wor]=val\n",
    "            else :\n",
    "                 word_weight_synopsis[wor]=val\n",
    "\n",
    "            j=j+1\n",
    "            \n",
    "    # evaluation\n",
    "    return (word_weight,word_weight_synopsis)\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics=100\n",
    "passes=50\n",
    "(word_weight,word_weight_synopsis)=solve(total_topics,corpus,id_word_dict,corpus2,id_word_dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5318940137389597, 0.6241413150147204, 0.6967615309126595, 0.7497546614327772, 0.7978410206084396, 0.8321884200196271, 0.858684985279686, 0.8753680078508341, 0.8969578017664377, 0.9106967615309126]\n",
      "[0.208955223880597, 0.2293762575452716, 0.24817518248175183, 0.2693409742120344, 0.2746478873239437, 0.27848101265822783, 0.25, 0.22085889570552147, 0.23357664233576642, 0.24793388429752067]\n"
     ]
    }
   ],
   "source": [
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "f1 = []\n",
    "acc =[]\n",
    "threshold=0.08\n",
    "threshold2=0.02\n",
    "x=0\n",
    "for x in range(10):\n",
    "    #print(word_weight_synopsis)\n",
    "    (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)=test_model(word_weight,word_weight_synopsis,threshold,threshold2)\n",
    "#     print(total_size)\n",
    "    tp.append(true_positive)\n",
    "    tn.append(false_positive)\n",
    "    fp.append(true_negative)\n",
    "    fn.append(false_negative)\n",
    "    f1.append(f1score)\n",
    "    acc.append(accuracy)\n",
    "    threshold+=0.01\n",
    "\n",
    "print(acc)\n",
    "print(f1)\n",
    "sample = open('godfather_result.txt',\"a\")\n",
    "print(\"iteration:\\n\",x,file = sample)\n",
    "print(\"\\ntopic:\",total_topics,file=sample)\n",
    "print( \"\\naccuracy:\\n\",acc , file = sample)\n",
    "print( \"\\ntrue_positive:\\n\",tp , file = sample)\n",
    "print( \"\\nfalse_positive:\\n\",fp , file = sample)\n",
    "print( \"\\ntrue_negative:\\n\",tn , file = sample)\n",
    "print( \"\\nfalse_negative:\\n\",fn , file = sample)\n",
    "print( \"\\nf1-score:\\n\",f1 ,file = sample)\n",
    "sample.close() \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 828 108 54 0.2636363636363636 0.8410206084396468\n"
     ]
    }
   ],
   "source": [
    "print(true_positive,true_negative,false_positive,false_negative,f1score,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
