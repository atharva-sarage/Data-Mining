{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Synonyms\n",
    "It is an improvisation to the plain LDA model. In this model while comparing a word in a comment to the synopsis we also search for its similars words which are its synonyms. Wordnet Libray from NLTK library is used to get the synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 1500\n"
     ]
    }
   ],
   "source": [
    "#comments dataset path (just change file name for different movies testing)\n",
    "dataset_file=\"dataset_godfather.csv\"\n",
    "#synopsis text file path (just change file name for different movies testing)\n",
    "synopsis_file='synopsis_godfather.txt'\n",
    "df = pd.read_csv(dataset_file)\n",
    "file = open(synopsis_file,\"r\")\n",
    "text=file.read()\n",
    "sentences = text.split('. ')\n",
    "   \n",
    "df2 = pd.DataFrame(sentences, columns=[\"synopsis\"])\n",
    "df2.to_csv('synopsis3.csv', index=False)\n",
    "# print(df2.head())\n",
    "df_pos=df.loc[df['is_spoiler'] == True]\n",
    "df_neg=df.loc[df['is_spoiler'] == False]\n",
    "pos_size=min(1000,df_pos.shape[0])\n",
    "neg_size=min(1500,df_neg.shape[0])\n",
    "print(pos_size,neg_size)\n",
    "#Splitting dataset into train and test in 8:2 ratio\n",
    "train_size=0.8*(pos_size+neg_size)\n",
    "test_size=0.2*(pos_size+neg_size)\n",
    "df_train_pos=df_pos.loc[:0.8*pos_size,:]\n",
    "df_test_pos=df_pos.loc[0.8*pos_size+1:,:]\n",
    "df_train_neg=df_neg.loc[:0.8*neg_size,:]\n",
    "df_test_neg=df_neg.loc[0.8*neg_size+1:,:]\n",
    "# df_train_neg.to_csv('train.csv', index=False)\n",
    "frames_train=[df_train_pos,df_train_neg]\n",
    "frames_test=[df_test_pos,df_test_neg]\n",
    "df_train=pd.concat(frames_train)\n",
    "df_test=pd.concat(frames_test)\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size=len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending Comment before index number in index column\n",
    "index=[]\n",
    "for i in range(len(df_train.index)):\n",
    "    index.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df_train['index']=index\n",
    "df_train.set_index('index', inplace=True)\n",
    "\n",
    "index2=[]\n",
    "for i in range(len(df2.index)):\n",
    "    index2.append(\"Comment \"+str(i))\n",
    "#df.insert(loc=0,column='index',value=index)\n",
    "df2['index']=index2\n",
    "df2.set_index('index', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing \n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning both comments and synopsis data\n",
    "data_clean = pd.DataFrame(df_train.review_text.apply(round1))\n",
    "data_clean2 = pd.DataFrame(df2.synopsis.apply(round1))\n",
    "# data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 948)\n",
      "(1118, 14087)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Creating document term matrix for comments data with each entry denoting the count of the term in the comment\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.review_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "#Creating document term matrix for synopsis data with each entry denoting the count of the term in the comment\n",
    "cv2 = CountVectorizer(stop_words='english')\n",
    "data_cv2 = cv2.fit_transform(data_clean2.synopsis)\n",
    "data_dtm2 = pd.DataFrame(data_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "data_dtm2.index = data_clean2.index\n",
    "print(data_dtm2.shape)\n",
    "print(data_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating id to word dictionary for comments data\n",
    "id_word_dict={}\n",
    "for i in range(len(data_dtm.columns)):\n",
    "    id_word_dict[i]=data_dtm.columns[i]\n",
    "#print(id_word_dict)\n",
    "tdm = data_dtm.transpose()\n",
    "\n",
    "#Creating id to word dictionary for synopsis data\n",
    "id_word_dict2={}\n",
    "for i in range(len(data_dtm2.columns)):\n",
    "    id_word_dict2[i]=data_dtm2.columns[i]\n",
    "# print(id_word_dict2)\n",
    "tdm2 = data_dtm2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the term document matrix in sparse matrix for preparing corpus of comments data\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "#Converting the term document matrix in sparse matrix for preparing corpus of synopsis data\n",
    "sparse_counts2 = scipy.sparse.csr_matrix(tdm2)\n",
    "corpus2 = matutils.Sparse2Corpus(sparse_counts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model.\n",
    "###  Cosine similarity is used as a measure of similarity. A word is valid if it occurs in both comment and synopsis and with some threshold in the synopsis.\n",
    "### The comment is considered as spoiler if its cosine similarity value crosses some threshold which is a hyperameter for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(word_weight,word_weight_synopsis,threshold,threshold2):\n",
    "    df3=pd.read_csv(\"test.csv\")\n",
    "    total_size=df3.shape[0]\n",
    "    \n",
    "    output_list=[]\n",
    "    counter=0\n",
    "    for j in range(total_size):\n",
    "        comment_t=df3['review_text'][j]\n",
    "        if df3['is_spoiler'][j] == 0:\n",
    "            counter+=1\n",
    "\n",
    "        comment_t=clean_text_round1(comment_t)\n",
    "        s1=comment_t.split(\" \")\n",
    "        comment_ts=set(s1)\n",
    "        comment_ts\n",
    "        sum=0\n",
    "        sum2=0\n",
    "        sum3=0\n",
    "        for i in word_weight_synopsis.values():\n",
    "            sum3+=float(i)*float(i)  \n",
    "\n",
    "        for wor in comment_ts:\n",
    "            temp_max=0\n",
    "            # populate the synonym list for each word and check it in the synopsis\n",
    "            synonyms = []\n",
    "            synonyms.append(wor);\n",
    "            for syn in wordnet.synsets(wor): \n",
    "                for l in syn.lemmas(): \n",
    "                    synonyms.append(l.name()) \n",
    "            flag=False\n",
    "            found_word=\"\";\n",
    "            for i in synonyms: \n",
    "                if i in word_weight :\n",
    "                    found_word=i\n",
    "                    flag=True                \n",
    "                if i in word_weight_synopsis and i in word_weight and float(word_weight_synopsis[i])>threshold2:\n",
    "                    temp_max=max(temp_max,float(word_weight_synopsis[i])*float(word_weight[i]))\n",
    "                    found_word=i\n",
    "                    break\n",
    "\n",
    "            if flag==True:\n",
    "                sum2+=float(word_weight[found_word])*float(word_weight[found_word])\n",
    "                sum+=temp_max\n",
    "\n",
    "        if sum2!=0 :\n",
    "            #print(sum,sum2,sum3)\n",
    "            output_list.append(sum/(np.sqrt(sum2)*np.sqrt(sum3)))\n",
    "        else:\n",
    "            output_list.append(0)\n",
    "            \n",
    "            \n",
    "    df3['output']=output_list\n",
    "    cols = list(df3.columns.values)\n",
    "    df3 = df3[['Unnamed: 0', 'is_spoiler',  'output' ,'review_text']]\n",
    "    df3.to_csv('final.csv')\n",
    "    total=0\n",
    "    false_positive=0\n",
    "    false_negative=0\n",
    "    true_positive=0\n",
    "    true_negative=0\n",
    "\n",
    "    # calculating the result of model and various parametrs such as fp,fn,tp,tn;\n",
    "    for i in range(total_size):\n",
    "        #print(df3['output'][i],threshold,df3['Y'][i])\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==True  :\n",
    "            true_positive+=1\n",
    "        if  df3['output'][i]<threshold and df3['is_spoiler'][i]==False :\n",
    "            true_negative+=1\n",
    "        if df3['output'][i]>threshold and df3['is_spoiler'][i]==False:\n",
    "            false_positive+=1\n",
    "        if df3['output'][i]<threshold and df3['is_spoiler'][i]==True:\n",
    "            false_negative+=1\n",
    "    \n",
    "    accuracy=(true_negative+true_positive)/total_size\n",
    "    precision=true_positive/(true_positive+false_positive)\n",
    "    recall=true_positive/(true_positive+false_negative)\n",
    "    f1score=(2*precision*recall)/(precision+recall)\n",
    "    return (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve function takes corpus, dictionary ,passes as arguments. It then trains the 2 lda models on the given parameters and returns word weight dictionry for both comments and synopsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(total_topics,corpus,id_word_dict,corpus2,id_word_dict2,passes):\n",
    "    lda = models.LdaMulticore(corpus=corpus, id2word=id_word_dict, num_topics=total_topics, passes=passes)\n",
    "    lda2 = models.LdaMulticore(corpus=corpus2, id2word=id_word_dict2, num_topics=total_topics, passes=passes)\n",
    "    \n",
    "#storing dictionary of important words obtained by LDA on comments data and storing the maximum probability\n",
    "#of that word occuring in any topic\n",
    "# word weight dictionary comments\n",
    "    word_weight={}    \n",
    "    for i in range(total_topics):\n",
    "        word_topic=lda.print_topic(i).split('+')\n",
    "        j=0\n",
    "        for word_probab in word_topic:\n",
    "            val,wor=word_probab.split('*')\n",
    "            wor=str(wor)\n",
    "            y=len(wor)\n",
    "            if j==len(word_topic)-1:\n",
    "                wor = wor[1:y-1]\n",
    "            else:\n",
    "                wor = wor[1:y-2]\n",
    "            if wor in word_weight:\n",
    "                if word_weight[wor]<val:\n",
    "                    word_weight[wor]=val\n",
    "            else :\n",
    "                 word_weight[wor]=val\n",
    "            j=j+1\n",
    "            \n",
    "#storing dictionary of important words obtained by LDA on synopsis data and storing the maximum probability\n",
    "#of that word occuring in any topic\n",
    "# word weight dictionary synopsis\n",
    "    word_weight_synopsis={}\n",
    "    for i in range(total_topics):\n",
    "        word_topic2=lda2.print_topic(i).split('+')\n",
    "        j=0\n",
    "        for word_probab in word_topic2:\n",
    "            val,wor=word_probab.split('*')        \n",
    "            wor=str(wor)\n",
    "            #print(wor)\n",
    "            y=len(wor)\n",
    "            if j==len(word_topic2)-1:\n",
    "                wor = wor[1:y-1]\n",
    "            else:\n",
    "                wor = wor[1:y-2]\n",
    "            #print(wor)\n",
    "            if wor in word_weight_synopsis:\n",
    "                if word_weight_synopsis[wor]<val:\n",
    "                    word_weight_synopsis[wor]=val\n",
    "            else :\n",
    "                 word_weight_synopsis[wor]=val\n",
    "\n",
    "            j=j+1\n",
    "            \n",
    "    # evaluation\n",
    "    return (word_weight,word_weight_synopsis)\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics=1\n",
    "passes=1\n",
    "(word_weight,word_weight_synopsis)=solve(total_topics,corpus,id_word_dict,corpus2,id_word_dict2,passes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of evaluation metrics for different values of threshold hyperparamete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-42feda8c391b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbest_threshold\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.11\u001b[0m \u001b[0;31m# add best threshold obtained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m  \u001b[0mdf4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_spoiler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrue_positive\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m  \u001b[0mdf4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_spoiler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluation metrices for top K comments.(The comments are ranked based on cosine similarity values)\n",
    "df4=pd.read_csv(\"final.csv\")\n",
    "df4.sort_values(by='output', ascending=False,inplace=True)\n",
    "top_k=100\n",
    "false_positive=0\n",
    "false_negative=0\n",
    "true_positive=0\n",
    "true_negative=0\n",
    "best_threshold= 0.11 # add best threshold obtained\n",
    "for i in range(top_k):\n",
    "    if  df4['output'][i]>threshold and df4['is_spoiler'][i]==True  :\n",
    "        true_positive+=1\n",
    "    if  df4['output'][i]<threshold and df4['is_spoiler'][i]==False :\n",
    "        true_negative+=1\n",
    "    if  df4['output'][i]>threshold and df4['is_spoiler'][i]==False:\n",
    "        false_positive+=1\n",
    "    if  df4['output'][i]<threshold and df4['is_spoiler'][i]==True:\n",
    "        false_negative+=1\n",
    "\n",
    "accuracy = (true_positive+true_negative)/(total_size)\n",
    "precision=true_positive/(true_positive+false_positive)\n",
    "recall=true_positive/(true_positive+false_negative)\n",
    "f1score=(2*precision*recall)/(precision+recall)\n",
    "print(accuracy,precision,recall,f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "f1 = []\n",
    "acc =[]\n",
    "threshold=0.08\n",
    "threshold2=0.02\n",
    "x=0\n",
    "for x in range(10):\n",
    "    #print(word_weight_synopsis)\n",
    "    (true_positive,true_negative,false_positive,false_negative,f1score,accuracy)=test_model(word_weight,word_weight_synopsis,threshold,threshold2)\n",
    "#     print(total_size)\n",
    "    tp.append(true_positive)\n",
    "    tn.append(false_positive)\n",
    "    fp.append(true_negative)\n",
    "    fn.append(false_negative)\n",
    "    f1.append(f1score)\n",
    "    acc.append(accuracy)\n",
    "    threshold+=0.01\n",
    "\n",
    "print(acc)\n",
    "print(f1)\n",
    "sample = open('godfather_result.txt',\"a\")\n",
    "print(\"iteration:\\n\",x,file = sample)\n",
    "print(\"\\ntopic:\",total_topics,file=sample)\n",
    "print( \"\\naccuracy:\\n\",acc , file = sample)\n",
    "print( \"\\ntrue_positive:\\n\",tp , file = sample)\n",
    "print( \"\\nfalse_positive:\\n\",fp , file = sample)\n",
    "print( \"\\ntrue_negative:\\n\",tn , file = sample)\n",
    "print( \"\\nfalse_negative:\\n\",fn , file = sample)\n",
    "print( \"\\nf1-score:\\n\",f1 ,file = sample)\n",
    "sample.close() \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_positive,true_negative,false_positive,false_negative,f1score,accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
